---
title: 'AERO: preliminary analysis of Niger measles data, 1995-2005'
output: html_document
---

```{r, echo=F}
d<-read.csv("niger_regional_1995_2005.csv",header=T)
```

```{r}
dim(d)
rownames(d)<-d[,1]
d<-d[,-1]
rownames(d)
```

The data frame contains 40 regions and 572 time points spanning 11 years (presumably weeks starting 1 Jan 1995, since 11*nw=572). First, transpose the data frame and name the weeks 1 through 572

```{r}
d<-as.data.frame(t(d))
rownames(d)<-1:572
```

Each entry in the data frame is (presumably) the number of reported measles cases in that region in that week. Now we add a column for the country-level reported measles cases and inspect this

```{r}
d$national<-rowSums(d,na.rm=T)
plot(d$national,type="l",col="red")
```

We observe a seaonal (annual) incidence pattern with considerable variation in total cases per year. Many regions have >1 year intervals between large outbreaks. This is posited to be due to weak coupling between regions, resulting in a distribution of import times >1 year. During this time, regions may be building up susceptible numbers (mainly through births) driving the system to criticality. 

```{r}
fromto<-matrix(c(130,210,1,1,100,350,100,210,100,200,100,200,100,200,100,300,50,250,1,1,1,1,100,250,150,200,150,200,150,300,150,300,150,250,150,200,100,200,50,150,150,300,100,200,50,450,50,250,50,100,1,1,1,1,350,420,250,300,50,200,100,250,250,300,1,1,250,300,150,200,250,300,250,300,100,200,250,300,50,200),ncol=2,byrow=T)
par(mfrow=c(10,4),mar=c(1,1,1,1))
for (i in 1:40){
  plot(d[,i],type="l",col="blue")
  abline(v=fromto[i,1],col="red")
  abline(v=fromto[i,2],col="red")
}
```

Obtain drift time series and randomized versions of same. Drift is time series we suspect may have $R_0$ drifting towards 1. They are the red intervals in blue time series above, excluding those with NA, or exclusion by visual inspection (e.g annual peaks), {currently not doing this: or if very few cases (max incidence <10)}.

```{r}
lappend <- function(lst, obj) {
  lst[[length(lst)+1]] <- obj
  return(lst)
}
inter.epi.2<-list()
for (i in 1:40){
  if ((is.na(min(d[,i]))==F)&(fromto[i,1]>1)&(max(d[fromto[i,1]:fromto[i,2],i])>=1)){
  this.ts<-d[fromto[i,1]:fromto[i,2],i]
  #this.ts<-tail(this.ts,50)
  if (length(this.ts)>80){
  inter.epi.2<-lappend(inter.epi.2,this.ts)
  }
  }
}
# run this line to subset list based on cumulative cases in TS>=100
inter.epi.2<-inter.epi.2[sapply(inter.epi.2,function(x) sum(x)>=100)]

inter.epi.2.seasonal<-inter.epi.2
#FORMAL detrending here...
library(forecast)
for (i in 1:length(inter.epi.2.seasonal)){
  trend<-ma(unlist(inter.epi.2.seasonal[i]),order=52,centre=T)
  inter.epi.2[i]<-unlist(inter.epi.2.seasonal[i])-as.numeric(trend)
}

z<-inter.epi.2[[10]]
plot(z,type="l",col="red")
z.stl<-stl(ts(z,freq=52),s.window="per",robust=F)
plot(z.stl)


chop<-2

if (chop==1){
non.inter.epi.2<-list() #chops up time series
for (i in 1:length(inter.epi.2)){
  new.order<-sample(1:length(inter.epi.2[[i]]),replace=F)
  scrambled<-inter.epi.2[[i]][order(new.order)]
  non.inter.epi.2<-lappend(non.inter.epi.2,scrambled)
}
}
if (chop==2){
### ALTERNATE CHOP: distribute cases randomly among weeks of the time series segment
non.inter.epi.2<-list()
for (i in 1:length(inter.epi.2)){
  n.cases<-sum(inter.epi.2[[i]])
  n.weeks<-length(inter.epi.2[[i]])
  non.inter.epi.2<-lappend(non.inter.epi.2,hist(sample(1:n.weeks,n.cases,replace=T),breaks=seq(0.5,n.weeks+0.5,1),plot=F)$counts)
}
}


```

Now we see if the Kendall's tau statistic for a given EWS statistic is different for time series of the two lists

```{r, warning=F}
library(VGAM)
library(spaero)
cf.tau<-NULL
  bw.frac<-0.25 # fraction of TS used as bandwidth
ny<-1
for (i in 1:length(inter.epi.2)){
  nw<-length(inter.epi.2[[i]])
  st1<-get_stats(inter.epi.2[[i]],center_kernel="uniform",center_trend="local_constant",center_bandwidth=bw.frac*nw,stat_bandwidth=bw.frac*nw)
  cf.tau<-rbind(cf.tau,c(1,kendall.tau(seq(1,ny*nw,1),st1$stats$variance),kendall.tau(seq(1,ny*nw,1),st1$stats$autocovariance),kendall.tau(seq(1,ny*nw,1),st1$stats$autocorrelation),kendall.tau(seq(1,ny*nw,1),st1$stats$decay_time),kendall.tau(seq(1,ny*nw,1),st1$stats$mean),kendall.tau(seq(1,ny*nw,1),st1$stats$index_of_dispersion),kendall.tau(seq(1,ny*nw,1),st1$stats$coefficient_of_variation),kendall.tau(seq(1,ny*nw,1),st1$stats$skewness),kendall.tau(seq(1,ny*nw,1),st1$stats$kurtosis)))
}
for (i in 1:length(non.inter.epi.2)){
  nw<-length(non.inter.epi.2[[i]])
  st1<-get_stats(non.inter.epi.2[[i]],center_kernel="uniform",center_trend="local_constant",center_bandwidth=bw.frac*nw,stat_bandwidth=bw.frac*nw)
  cf.tau<-rbind(cf.tau,c(0,kendall.tau(seq(1,ny*nw,1),st1$stats$variance),kendall.tau(seq(1,ny*nw,1),st1$stats$autocovariance),kendall.tau(seq(1,ny*nw,1),st1$stats$autocorrelation),kendall.tau(seq(1,ny*nw,1),st1$stats$decay_time),kendall.tau(seq(1,ny*nw,1),st1$stats$mean),kendall.tau(seq(1,ny*nw,1),st1$stats$index_of_dispersion),kendall.tau(seq(1,ny*nw,1),st1$stats$coefficient_of_variation),kendall.tau(seq(1,ny*nw,1),st1$stats$skewness),kendall.tau(seq(1,ny*nw,1),st1$stats$kurtosis)))
}

cf.tau<-as.data.frame(cf.tau)
names(cf.tau)<-c("label",names(st1$stats))
cf.tau.orig<-cf.tau
my.aucs<-NULL
for (i in 1:9){
  tmp<-cf.tau[,c(1,(i+1))]
  real.name<-names(tmp)[2]
  names(tmp)[2]<-"stat.of.interest"
  tmp<-tmp[order(tmp$stat.of.interest),]
  pos1<-which(tmp$label==1)#position in ranked list of 1's
  n1<-length(pos1)#number of 1's in vector
  pos0<-which(tmp$label==0)
  n0<-length(pos0)
  auc<-0
  for (j in 1:n1){
    auc<-auc+(pos1[j]-j)
  }
  auc<-auc/(n0*n1)
  my.aucs<-rbind(my.aucs,c(real.name,auc))
}

my.aucs<-as.data.frame(my.aucs)
names(my.aucs)<-c("EWS","AUC")
my.aucs$AUC<-as.numeric(as.character(my.aucs$AUC))

```

```{r, eval=T, warning=F}
bstrap.num<-100

for (b in 1:(bstrap.num-1)){# add to original to make n=10,100 bootstrap
  
inter.epi.3<-inter.epi.2

if (chop==1){

non.inter.epi.2<-list() #chops up time series
for (i in 1:length(inter.epi.2)){
  new.order<-sample(1:length(inter.epi.2[[i]]),replace=F)
  scrambled<-inter.epi.2[[i]][order(new.order)]
  non.inter.epi.2<-lappend(non.inter.epi.2,scrambled)
}
}

if (chop==2){
### ALTERNATE CHOP: distribute cases randomly among weeks of the time series segment
non.inter.epi.2<-list()
for (i in 1:length(inter.epi.2)){
  n.cases<-sum(inter.epi.2[[i]])
  n.weeks<-length(inter.epi.2[[i]])
  non.inter.epi.2<-lappend(non.inter.epi.2,hist(sample(1:n.weeks,n.cases,replace=T),breaks=seq(0.5,n.weeks+0.5,1),plot=F)$counts)
}
}
non.inter.epi.3<-non.inter.epi.2

cf.tau<-NULL
for (i in 1:length(inter.epi.3)){
  nw<-length(inter.epi.3[[i]])
  st1<-get_stats(inter.epi.3[[i]],center_kernel="uniform",center_trend="local_constant",center_bandwidth=bw.frac*nw,stat_bandwidth=bw.frac*nw)
  cf.tau<-rbind(cf.tau,c(1,kendall.tau(seq(1,ny*nw,1),st1$stats$variance),kendall.tau(seq(1,ny*nw,1),st1$stats$autocovariance),kendall.tau(seq(1,ny*nw,1),st1$stats$autocorrelation),kendall.tau(seq(1,ny*nw,1),st1$stats$decay_time),kendall.tau(seq(1,ny*nw,1),st1$stats$mean),kendall.tau(seq(1,ny*nw,1),st1$stats$index_of_dispersion),kendall.tau(seq(1,ny*nw,1),st1$stats$coefficient_of_variation),kendall.tau(seq(1,ny*nw,1),st1$stats$skewness),kendall.tau(seq(1,ny*nw,1),st1$stats$kurtosis)))
}
for (i in 1:length(non.inter.epi.3)){
  nw<-length(non.inter.epi.3[[i]])
  st1<-get_stats(non.inter.epi.3[[i]],center_kernel="uniform",center_trend="local_constant",center_bandwidth=bw.frac*nw,stat_bandwidth=bw.frac*nw)
  cf.tau<-rbind(cf.tau,c(0,kendall.tau(seq(1,ny*nw,1),st1$stats$variance),kendall.tau(seq(1,ny*nw,1),st1$stats$autocovariance),kendall.tau(seq(1,ny*nw,1),st1$stats$autocorrelation),kendall.tau(seq(1,ny*nw,1),st1$stats$decay_time),kendall.tau(seq(1,ny*nw,1),st1$stats$mean),kendall.tau(seq(1,ny*nw,1),st1$stats$index_of_dispersion),kendall.tau(seq(1,ny*nw,1),st1$stats$coefficient_of_variation),kendall.tau(seq(1,ny*nw,1),st1$stats$skewness),kendall.tau(seq(1,ny*nw,1),st1$stats$kurtosis)))
}

cf.tau<-as.data.frame(cf.tau)
names(cf.tau)<-c("label",names(st1$stats))

my.aucs.2<-NULL
for (i in 1:9){
  tmp<-cf.tau[,c(1,(i+1))]
  real.name<-names(tmp)[2]
  names(tmp)[2]<-"stat.of.interest"
  tmp<-tmp[order(tmp$stat.of.interest),]
  pos1<-which(tmp$label==1)#position in ranked list of 1's
  pos0<-which(tmp$label==0)
  n1<-length(pos1)#number of 1's in vector
  n0<-length(pos0)
  auc<-0
  for (j in 1:n1){
    auc<-auc+(pos1[j]-j)
  }
  auc<-auc/(n0*n1)
  my.aucs.2<-rbind(my.aucs.2,c(real.name,auc))
}
my.aucs.2<-as.data.frame(my.aucs.2)
names(my.aucs.2)<-c("EWS","AUC")
my.aucs.2$AUC<-as.numeric(as.character(my.aucs.2$AUC))
my.aucs<-merge(my.aucs,my.aucs.2,by="EWS")

}

par(mfrow=c(1,1),mar=c(5,5,5,1))

ews.list<-list()
my.aucs.vals<-t(my.aucs[,2:(bstrap.num+1)])
for (i in 1:9){
  ews.list<-lappend(ews.list,unname(my.aucs.vals[,i]))
}

boxplot(ews.list,names=NA,cex.axis=1.0,las=1,ylim=c(0,1),col="gray",ylab="AUC")
lines(c(0.12,9.88),c(0.5,0.5),col="red")
text(1:9,rep(0.2,9),my.aucs$EWS,srt=90,col="gray20",cex=0.7)
```


quick test to see how common 0 cases per week is each year by region
```{r}
prop.0<-NULL
burden<-NULL
for (i in 1:dim(d)[2]){
  e<-subset(d,select=colnames(d)[i])
  names(e)<-"cases"
  e$year<-as.integer(rownames(e))%/%52
  ag0<-aggregate(e$cases,by=list(e$year),FUN=function(x){length(which(x==0))/length(x)})
  agburden<-aggregate(e$cases,by=list(e$year),FUN=sum)
  prop.0<-rbind(prop.0,ag0)
  burden<-rbind(burden,agburden)
}
mean(prop.0$x)
```

check frequency of extinctions
```{r}
extinctions<-data.frame(city=factor(levels=colnames(d)),pwks0=double(0))
for (i in 1:40){
  extinctions[i,1]<-colnames(d)[i]
  extinctions[i,2]<-length(which(d[,i]==0))/length(which(is.na(d[,1])==F))
}
```


look at interepidemic time series data
```{r, eval=F}
for (j in 1:3){
  par(mfrow=c(5,2))
  for (i in 1:10){
idx<-j*i
plot(inter.epi.2[[idx]],type="l",xlim=c(0,400),ylim=c(0,100),ylab="cases",xlab="time (weeks)",main="interepidemic time series data")
}
}

6,7,8,9,10,13,14,15,16,17,18,19

for (i in 1:19){
  print(sum(inter.epi.2[[i]]))
}

```

